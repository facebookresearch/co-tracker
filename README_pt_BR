# CoTracker3: Rastreamento de Pontos Mais Simples e Melhorado por Pseudo-Rotulagem de V√≠deos Reais

**[Meta AI Research, GenAI](https://ai.facebook.com/research/)**; **[University of Oxford, VGG](https://www.robots.ox.ac.uk/~vgg/)**

[Nikita Karaev](https://nikitakaraevv.github.io/), [Iurii Makarov](https://linkedin.com/in/lvoursl), [Jianyuan Wang](https://jytime.github.io/), [Ignacio Rocco](https://www.irocco.info/), [Benjamin Graham](https://ai.facebook.com/people/benjamin-graham/), [Natalia Neverova](https://nneverova.github.io/), [Andrea Vedaldi](https://www.robots.ox.ac.uk/~vedaldi/), [Christian Rupprecht](https://chrirupp.github.io/)

### [P√°gina do Projeto](https://cotracker3.github.io/) | [Artigo #1](https://arxiv.org/abs/2307.07635) | [Artigo #2](https://arxiv.org/abs/2410.11831) | [T√≥pico no X](https://twitter.com/n_karaev/status/1742638906355470772) | [BibTeX](#citing-cotracker)

<a target="_blank" href="https://colab.research.google.com/github/facebookresearch/co-tracker/blob/main/notebooks/demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir no Colab"/>
</a>
<a href="https://huggingface.co/spaces/facebook/cotracker">
  <img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>

<img width="1100" src="./assets/teaser.png" />

**CoTracker** √© um modelo r√°pido baseado em transformadores que pode rastrear qualquer ponto em um v√≠deo. Ele traz para o rastreamento alguns dos benef√≠cios do Fluxo √ìptico.

CoTracker pode rastrear:

- **Qualquer pixel** em um v√≠deo
- Um conjunto **quase denso** de pixels em conjunto
- Pontos que podem ser selecionados manualmente ou amostrados em uma grade em qualquer quadro de v√≠deo

Experimente esses modos de rastreamento com nosso [demo no Colab](https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb) ou no [Espa√ßo Hugging Face ü§ó](https://huggingface.co/spaces/facebook/cotracker).

**Atualiza√ß√µes:**

- [15 de outubro de 2024] üì£ Estamos lan√ßando o CoTracker3! Rastreamento de pontos de √∫ltima gera√ß√£o com uma arquitetura leve, treinada com 1000 vezes menos dados do que os modelos de melhor desempenho anteriores. C√≥digo para modelos de refer√™ncia e o pipeline de pseudo-rotulagem est√£o dispon√≠veis no reposit√≥rio, assim como os pontos de verifica√ß√£o do modelo. Confira nosso [artigo](https://arxiv.org/abs/2410.11831) para mais detalhes.

- [25 de setembro de 2024] CoTracker2.1 est√° dispon√≠vel! Este modelo tem um desempenho melhor nos benchmarks TAP-Vid e segue a arquitetura do CoTracker original. Experimente!

- [14 de junho de 2024] Lan√ßamos o c√≥digo para [VGGSfM](https://github.com/facebookresearch/vggsfm), um modelo para recuperar poses de c√¢mera e estrutura 3D a partir de sequ√™ncias de imagens com base no rastreamento de pontos! VGGSfM √© o primeiro framework SfM totalmente diferenci√°vel que desbloqueia escalabilidade e supera m√©todos convencionais de SfM em benchmarks padr√£o. 

- [27 de dezembro de 2023] CoTracker2 est√° agora dispon√≠vel! Ele agora pode rastrear muitos mais (at√© **265*265**!) pontos conjuntamente e possui uma implementa√ß√£o mais limpa e eficiente em termos de mem√≥ria. Tamb√©m oferece suporte a processamento online. Consulte o [artigo atualizado](https://arxiv.org/abs/2307.07635) para mais detalhes. A vers√£o antiga permanece dispon√≠vel [aqui](https://github.com/facebookresearch/co-tracker/tree/8d364031971f6b3efec945dd15c468a183e58212).

- [5 de setembro de 2023] Agora voc√™ pode executar nosso demo do Gradio [localmente](./gradio_demo/app.py).

## In√≠cio R√°pido
A maneira mais f√°cil de usar o CoTracker √© carregar um modelo pr√©-treinado de `torch.hub`:

### Modo Offline: 
```pip install imageio[ffmpeg]```, depois:
```python
import torch
# Baixar o v√≠deo
url = 'https://github.com/facebookresearch/co-tracker/raw/refs/heads/main/assets/apple.mp4'

import imageio.v3 as iio
frames = iio.imread(url, plugin="FFMPEG")  # plugin="pyav"

device = 'cuda'
grid_size = 10
video = torch.tensor(frames).permute(0, 3, 1, 2)[None].float().to(device)  # B T C H W

# Executar CoTracker Offline:
cotracker = torch.hub.load("facebookresearch/co-tracker", "cotracker3_offline").to(device)
pred_tracks, pred_visibility = cotracker(video, grid_size=grid_size) # B T N 2,  B T N 1
```
### Modo Online: 
```python
cotracker = torch.hub.load("facebookresearch/co-tracker", "cotracker3_online").to(device)

# Executar CoTracker Online, o mesmo modelo com uma API diferente:
# Inicializar processamento online
cotracker(video_chunk=video, is_first_step=True, grid_size=grid_size)  

# Processar o v√≠deo
for ind in range(0, video.shape[1] - cotracker.step, cotracker.step):
    pred_tracks, pred_visibility = cotracker(
        video_chunk=video[:, ind : ind + cotracker.step * 2]
    )  # B T N 2,  B T N 1
```
O processamento online √© mais eficiente em termos de mem√≥ria e permite o processamento de v√≠deos mais longos. No entanto, no exemplo acima, o comprimento do v√≠deo √© conhecido! Consulte [o demo online](./online_demo.py) para um exemplo de rastreamento a partir de uma transmiss√£o online com um comprimento de v√≠deo desconhecido.

### Visualizar Trilhas Previstos: 
Ap√≥s [instalar](#installation-instructions) o CoTracker, voc√™ pode visualizar as trilhas com:
```python
from cotracker.utils.visualizer import Visualizer

vis = Visualizer(save_dir="./saved_videos", pad_value=120, linewidth=3)
vis.visualize(video, pred_tracks, pred_visibility)
```

Oferecemos v√°rias outras formas de interagir com o CoTracker:

1. Demo interativo do Gradio:
   - Um demo est√° dispon√≠vel no [Espa√ßo Hugging Face ü§ó `facebook/cotracker`](https://huggingface.co/spaces/facebook/cotracker).
   - Voc√™ pode usar o demo do Gradio localmente executando [`python -m gradio_demo.app`](./gradio_demo/app.py) ap√≥s instalar os pacotes necess√°rios: `pip install -r gradio_demo/requirements.txt`.
2. Notebook Jupyter:
   - Voc√™ pode executar o notebook no
   [Google Colab](https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb).
   - Ou explorar o notebook localizado em [`notebooks/demo.ipynb`](./notebooks/demo.ipynb). 
2. Voc√™ pode [instalar](#installation-instructions) o CoTracker _localmente_ e ent√£o:
   - Executar um demo *offline* com 10 ‚®â 10 pontos amostrados em uma grade no primeiro quadro de um v√≠deo (os resultados ser√£o salvos em `./saved_videos/demo.mp4`)):

     ```bash
     python demo.py --grid_size 10
     ```
    - Executar um demo *online*:

      ```bash
      python online_demo.py
      ```

Uma GPU √© fortemente recomendada para usar o CoTracker localmente.

<img width="500" src="./assets/bmx-bumps.gif" />


## Instru√ß√µes de Instala√ß√£o
Voc√™ pode usar um modelo pr√©-treinado via PyTorch Hub, como descrito acima, ou instalar o CoTracker a partir deste reposit√≥rio GitHub.
Essa √© a melhor maneira se voc√™ precisar executar nosso demo local ou avaliar/treinar o CoTracker.

Certifique-se de ter _PyTorch_ e _TorchVision_ instalados em seu sistema. Siga as instru√ß√µes [aqui](https://pytorch.org/get-started/locally/) para a instala√ß√£o.
Recomendamos fortemente instalar PyTorch e TorchVision com suporte a CUDA, embora para pequenas tarefas o CoTracker possa ser executado na CPU.




### Instalar uma Vers√£o de Desenvolvimento

```bash
git clone https://github.com/facebookresearch/co-tracker
cd co-tracker
pip install -e .
pip install matplotlib flow_vis tqdm tensorboard
```

Voc√™ pode baixar manualmente todos os pontos de verifica√ß√£o do CoTracker3 (modelos base e escalados,

 `cotracker3_l.pth` e `cotracker3_s.pth`), que est√£o dispon√≠veis [nesta p√°gina](https://cotracker3.github.io).

## Citando o CoTracker
Por favor, cite CoTracker se voc√™ us√°-lo em sua pesquisa:
```bibtex
@inproceedings{karaev2023cotracker,
  title={CoTracker: It is Better to Track Together},
  author={Karaev, Nikita and Makarov, Iurii and Wang, Jianyuan and Rocco, Ignacio and Graham, Benjamin and Neverova, Natalia and Vedaldi, Andrea and Rupprecht, Christian},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year={2023}
}
@article{karaev2024cotracker3,
  title={CoTracker3: State-of-the-art Point Tracking with Minimalistic Architectures and Video Pseudo-labeling},
  author={Karaev, Nikita and Makarov, Iurii and Wang, Jianyuan and Rocco, Ignacio and Graham, Benjamin and Neverova, Natalia and Vedaldi, Andrea and Rupprecht, Christian},
  journal={arXiv preprint arXiv:2410.11831},
  year={2024}
}
```

